
\chapter{Graph Convolutional Networks - theory}

Graphs can defined as mathematical structures $G$ consisting of a set of vertices $V$, a set of edges $E$ and an incidence function $\phi$, along with many variations and generalizations to such structure, can be used for describing entities, which are related to each other in some way. An example of such model could be a computer network graph or citation network. Neurons can also be modelled in a similar way. Relation data can often be best described using such graphs. \cite{Lesk2024}

Some problems relating to such data can be solved using Convolutional Neural Networks -- this can also be the case for keystroke dynamics data, such as with Lu et al. \cite{Lu2020} or Sharma et al. \cite{Shar2023}. However, it can be reasoned that the Graph Neural Networks can also perform such tasks, with connections in graph data being used more directly in the model itself.

\section{Graph Neural Networks}
Graph Neural Networks (GNNs) are designed for graph inputs. The resulting outputs are also graphs (specifically, they are node embeddings representing a graph), allowing for transforming information in the graph's nodes, edges and global context, such as metadata about the graph, aggregated information, graph features etc. \cite{sanch2021}. GNN do not change the connectivity of the input in the output. 
TODO: adjacency matrix + node features, how agg. works + canon. formula, layers cause receptive field to widen significantly (important).

\section{Convolutional Networks and Graph Convolutional Networks}
Convolutional Neural Networks work with grid-like data structures, such as images, where each element (e.g., a pixel) has a defined spatial relationship with its neighbors (FIX SOURCE). An image can be treated as a special type of graph where each pixel is a node connected to its neighboring pixels by edges. This analogy helps in understanding that while CNNs process regular grids with a fixed neighborhood size, they can be seen as a specific case of Graph Neural Networks operating on structured data. This regular structure allows convolutional filters to move systematically across the input, helping to extract different levels of features. As a result, CNNs excel at computer vision tasks (FIX SOURCE). However, graphs do not provide the structured data layout required by CNNs. They can have varying numbers of neighbors and lack a consistent ordering, which makes applying CNNs directly ineffective for graph data.

Graph Convolutional Networks solve this problem by directly handling graph-structured data. Since nodes in graphs do not follow a specific order and can connect to different numbers of neighbors, GCNs use the graph's adjacency matrix to gather information from neighboring nodes. This method focuses on relationships between nodes (who is connected to whom) rather than their exact positions. Consequently, GCNs effectively capture both local and global structures in graphs, making them suitable for tasks like node classification, link prediction, and graph classification.


\section{Graph-level prediction in GNN}
Supervised learning on graphs can be achieved by labeling either nodes, edges or whole graphs. In typical training pipeline, an input graph is transformed into a node representation accepted by the network, which then transforms the data in the manner described above. Output node embeddings are then used to create a prediction head, which is used, together with labels and some loss function and evaluation metrics, for the prediction task. There are different prediction heads for node-level, edge-level or graph-level prediction \cite{Lesk2024}. For nodes, predictions can be made directly using node embeddings -- this can be done by using a classification layer, like a dense layer \cite{sanch2021}.  For the edges, this must be done on pairs of nodes. For global graph predictions a pooling of node embeddings can be performed. Options for pooling include for example global mean pooling, global max pooling or global sum pooling \cite{Lesk2024}.