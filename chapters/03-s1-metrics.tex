\section{Metrics}
Choosing a correct metric for a machine learning model is an important step for testing its performance. Furthermore, in the case of this project, a metric for testing the whole collections of models needs to be selected.
When considering the functioning of authentication system, these metrics are often used \cite{traore2011continuous}. 

\subsection{Accuracy}
Accuracy measures the proportion of correct predictions (both positive and negative) over all predictions. It is calculated as:
\[
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
\]
where:
\begin{itemize}
	\item $TP$: True Positives (correctly accepted genuine users),
	\item $TN$: True Negatives (correctly rejected imposters),
	\item $FP$: False Positives (incorrectly accepted imposters),
	\item $FN$: False Negatives (incorrectly rejected genuine users).
\end{itemize}

While accuracy is a commonly used metric, during the evaluation of models it was found that this score gave little information about the performance of the models. Most importantly, accuracy does not distinguish between false positives and false negatives, which from the perspective of this project differ in importance. False positives are considered more influential, as this kind of errors would allow imposters to gain access to the system, while false negatives would only force the user to repeat the authentication procedure.

\subsection{Precision and Recall (True Positive Rate, TPR)}
Precision measures the proportion of correctly predicted positive samples among all samples predicted as positive. It is given by:
\[
Precision = \frac{TP}{TP + FP}
\]

Recall measures the proportion of correctly predicted positive samples among all actual positive samples. It is calculated as:
\[
Recall = \frac{TP}{TP + FN}
\]

These measures are commonly used in machine learning and information retrieval setting, under the assumption that the negative class, and consequently, the number of true negatives, does not matter as much as the positive class. The focus on the correct recognition of the positive class matches the use function of models in this project.\\
As such, precision and recall were used when training models, to measure the performance on validation set and tuning hyperparameters.

\subsection{False Acceptance Rate (FAR) and False Rejection Rate (FRR)} \label{FAR_FRR_theory}
The False Acceptance Rate (FAR) represents the proportion of negative samples (imposters) that are incorrectly classified as positive. It is calculated as:
\[
FAR = \frac{FP}{FP + TN}
\]

The False Rejection Rate (FRR) represents the proportion of positive samples (genuine users) that are incorrectly classified as negative. It is calculated as:
\[
FRR = \frac{FN}{TP + FN}
\]

From the perspective of this project, FAR and FRR were considered to be the most important and informative metric. These metrics directly represent the security and usability of a system from the perspective of its users and therefore showcase the performance of the trained models in their intended. As such, the performance of the collection of models will be evaluated using these metrics. 

\subsection{Equal Error Rate (EER)}
In authentication systems, there is often a trade-off between FAR and FRR. By lowering the decision threshold, more users, both genuine and imposters would be authenticated, thus lowering FRR and increasing FAR. Conversely, a higher decision threshold would lead to a increased FRR and decreased FAR. \\
The Equal Error Rate is the value of these metrics at a threshold where FAR = FRR. If no such threshold can be found, EER can is calculated as  
\[
EER = \frac{FFR + FAR}{2}
\]
at a threshold where the difference between FFR + FAR is minimal.

The equal error rate is a useful metric which allow for easy comparison between different methods and applications. Naturally, the threshold used to measure the EER is not the optimal value for the perspective of the final application. As noted in the section about accuracy, the number of false negatives is the major concern for this project. Therefore, a higher false rejection rate is acceptable, if it results in a comparable drop in the false acceptance rate. Despite this fact, the EER will also be reported along with model performance, as it is a common metric used in authentication systems and allows comparing the outcomes to other findings using Keystroke Dynamics.
