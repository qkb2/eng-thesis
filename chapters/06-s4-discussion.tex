\section{Discussion}
The findings presented in the study can provide an insight into the developed model. It is evident that the model performed significantly worse for certain users compared to others. There could be many reasons for such behavior. One important fact is that some users wrote using more than one language at the time or wrote in a different language from the rest. Most users in the study wrote their training and testing samples in Polish only, but users such as 22, 24 and 42 did not. Of these, 24 and 42 wrote both in English and Polish. Of those, user 24 seems especially interesting, given that they wrote most of the training data in English, but their testing sample was written in Polish exclusively.

It should be reiterated that for this model it was especially important for the users to have the lowest possible False Acceptance Rate (FAR). In a theoretic system using such a model, a user being able to pass for another user is a more dangerous situation, than a user not being able to authenticate themselves. The second problem may be mitigated by user being authenticated with longer texts, or with multiple tries being averaged.

Some users are seemingly more similar to other users. For some especially problematic users, such as user 24, the likelihood of them being classified as user 21 far outweights the likelihood of them being classified as themselves. Interestingly, the reverse was not nearly as likely: user 21 was classified well and only rarely could be classifed as 24. This could be possibly explained by user 24 having their testing sample in a different language from the training data. Model seemed to classify this user as another Polish user because of that. User 22, who used English exclusively, was not problematic, thus giving some credence to this hypothesis.

Another problematic user was user 40, who wrote letter "y" for an extended period of time, thus invalidating much of the sample gathered, and also wrote in capital letters only for the last 300 characters. This user had relatively high False Rejection Rate of 0.459, possibly caused by the worse training data.

Users switching hands, writing some paragraphs with one hand and some with both or switching position such that their hand was performing differently (e.g., by lying on their side) could not be accounted for in this analysis - only stark changes in user's posture could be detected via the accelerometer, but these could also be explained by natural movements, which should not have any effect on the user's hand position relative to the screen. This problem is typical of mobile devices and should not exist in the computer and keyboard setting.

\subsection{Possible problems and further studies}
As previously stated, there could be many possible causes for such behavior. Many of such hypotheses could be tested, but a significantly larger number of participants would be required for such a study. For a truly significant test of the approach, each participant would also need to provide more data in the sample. This could prove challenging, since, according participant of this study, providing a sample as small as 1500 characters was already tiresome. Some users resulted to writing semantic noise, such as repeating one or two letter for an extended period of time, which made the data gathered from that part of a sample essentially useless. In some studies, such as AbdelTaouf et al. \cite{Abde2023}, this is somewhat mitigated by users typing some sentence or password repeatedly, but this study concerned itself with free-form writing. In other studies, such as Lu et al. \cite{Lu2020}, datasets with larger sample sizes were used, including the Clarkson II dataset with 103 users contributing 12.9 million keystrokes over 2.5 years, and the Buffalo dataset with 157 participants providing an average of 17,000 keystrokes across three sessions.

Possible way of gathering better quality data from more motivated participants could be construction of a chatbot of some sort, similar to a significantly simplified version of Replika \cite{replika} or Cleverbot \cite{cleverbot}, complete with a dedicated mobile application and a keyboard designed specifically for the app -- long use of the application would make users write more naturally over time, even on a foreign keyboard. The chatbot would entice the users to use the application more often, thus gathering more data. This approach, however, could lead to some security and privacy concerns. A limited approach is also possibly viable, with a simple chatGPT model being used as a conversational companion for the user only for a short duration, such as a 30min session.

There may be some fundamental problems with the data being gathered, as well. It may be that a limited set of features employed in this study is insufficient to perform as well as state of the art models, such as Lu et al. \cite{Lu2020}. Lu et al.'s model achieved an FAR of 2.83\% and 5.31\%, an FRR of 1.89\% and 6.61\%, and an EER of 2.36\% and 5.97\% for the Buffalo and Clarkson II datasets, respectively. In comparison, the model used in this study resulted in an FAR of 9.2\%, an FRR of 26.4\%, and an EER of 13.3\%. The observed performance differences may result from the dataset size and the available feature set. Expanding the feature set to include accelerometer data could improve performance but introduces several challenges. Collecting a significantly larger dataset would be necessary to cover various conditions, such as typing while sitting, lying down, or standing. This would help ensure that the model generalizes correctly across different scenarios.

True evaluation of the method is thus possible only with a larger sample size and text lengths. Gathering users proficient in different languages could also be helpful for analyzing the effects of two different languages being mixed in the model and possible adverse effects of such an approach. Different sets a letters used and different letter frequencies may lead to atypical data and problems in user recognition.