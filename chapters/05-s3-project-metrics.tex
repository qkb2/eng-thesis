\section{Metrics}
Choosing a correct metric for a machine learning model is an important step for testing its performance. Furthermore, in the case of this project, a metric for testing the whole collections of models needs to be selected.
When considering the functioning of authentication system, these metrics are often used \cite{traore2011continuous}. 

\subsubsection{1. Accuracy}
Accuracy measures the proportion of correct predictions (both positive and negative) over all predictions. It is calculated as:
\[
Accuracy = \frac{TP + TN}{TP + TN + FP + FN}
\]
where:
\begin{itemize}
	\item $TP$: True Positives (correctly accepted genuine users),
	\item $TN$: True Negatives (correctly rejected imposters),
	\item $FP$: False Positives (incorrectly accepted imposters),
	\item $FN$: False Negatives (incorrectly rejected genuine users).
\end{itemize}

While accuracy is a commonly used metric, during the evaluation of models we found that this score gave little information about the performance of the models. Most importantly, accuracy does not distinguish between positives and false negatives, which from the perspective of this project differ in importance. False positives are considered more influential, as these kinds of errors would allow imposters to gain access to the system, while false negatives would only force the user to repeat the authentication procedure.

\subsubsection{2. Precision and Recall (True Positive Rate, TPR)}
Precision measures the proportion of correctly predicted positive samples among all samples predicted as positive. It is given by:
\[
Precision = \frac{TP}{TP + FP}
\]

Recall measures the proportion of correctly predicted positive samples among all actual positive samples. It is calculated as:
\[
Recall = \frac{TP}{TP + FN}
\]

These measures are commonly used in machine learning and information retrieval setting, under the assumption that the negative class, and consequently, the number of true negatives, does not matter as much as the positive class. \myworries{Citation needed}. The focus on the correct recognition of the positive class matches the use function of models in this project.\\
As such, precision and recall were used when training models, to measure the performance on validation set and tuning hyperparameters.

\subsubsection{4. False Acceptance Rate (FAR) and False Rejection Rate (FRR)}
The False Acceptance Rate (FAR) represents the proportion of negative samples (imposters) that are incorrectly classified as positive. It is calculated as:
\[
FAR = \frac{FP}{FP + TN}
\]

The False Rejection Rate (FRR) represents the proportion of positive samples (genuine users) that are incorrectly classified as negative. It is calculated as:
\[
FRR = \frac{FN}{TP + FN}
\]

From the perspective of this project, FAR and FRR were considered to be the most important and informative metric. These metrics directly represent the security and usability of a system from the perspective of its users and therefore showcase the performance of the trained models in their intended. As such, the performance of the collection of models will be evaluated using these metrics. 

\subsubsection{6. Equal Error Rate (EER)}
In authentication systems, there is often a trade-off between FAR and FRR. By lowering the decision threshold, more users, both genuine and imposters would be authenticated, thus lowering FRR and increasing FAR. Conversely, a higher decision threshold would lead to a increased FRR and decreased FAR. \\
The Equal Error Rate is the value of these metrics at a threshold where FAR = FRR. If no such threshold can be found, EER can is calculated as  
\[
EER = \frac{FFR + FAR}{2}
\]
at a threshold where the difference between FFR + FAR is minimal.

The equal error rate is a useful metric which allow for easy comparison between different methods and applications. Naturally, the threshold used to measure the EER is not the optimal value for the perspective of the final application. As noted in the section about accuracy, the number of false negatives is the major concern for this project. Therefore, a higher false rejection rate is acceptable, if it results in a comparable drop in the false acceptance rate. Despite this fact, the EER will also be reported along with model performance, as it is a common metric used in authentication systems and allows comparing the outcomes to other findings using Keystroke Dynamics.


\section{Architecture}
The basis of a model for graph level prediction comprises of a graph convolutional layers, a pooling function to aggregate node information and a classification head, which outputs a single value, that is than passed to a sigmoid function. \myworries{Cite something} 
The architecture used in this project is pictured below.\\

\begin{figure}
\begin{center}
	\begin{tikzpicture}[node distance=1.5cm]
		
		% Nodes
		\node (gcn) [process] {GCNConv + ReLU \\ x2};
		\node (pooling) [process, below of=gcn] {Global Mean Pooling};
		\node (linear1) [process, below of=pooling] {Full Connected + ReLu};
		\node (dropout) [process, below of=linear1] {Dropout};
		\node (linear2) [process, below of=dropout] {Full Connected + Sigmoid};
		
		% Arrows
		\draw [arrow] (gcn) -- (pooling);
		\draw [arrow] (pooling) -- (linear1);
		\draw [arrow] (linear1) -- (dropout);
		\draw [arrow] (dropout) -- (linear2);
		
	\end{tikzpicture}
	
	\caption{Model architecture}
	\label{fig:model_arch}
\end{center}
\end{figure}


\myworries{Add some text to this drawing}

\subsubsection{GCN + ReLU}
The first three layers of the model, pictured in figure \ref{fig:model_arch}, are graph convolutional layers or GCNConv. The theory behind these layers was extensively discussed in chapter 3. After each layer a ReLU activation function was applied.

\subsubsection{Global Mean Pooling}
After two convolutional layers, a global mean pooling layer was applied to aggregate the node embeddings into one vector. 

\subsubsection{Fully Connected Layers}
The selected architecture uses two fully connected layers to transform the aggregated output into one value, 
which is passed through a sigmoid activation function. The first fully connected layer was added was added after empirical experimentation which showed an improvement in performance.

\subsubsection{Dropout}
A dropout layer was added between the two fully connected layers, which randomly disables some of the neurons during the training process. Such layers have been shown to create more robust models, that generalize better by Baldi et al \cite{NIPS2013_71f6278d}. Dropout layers have also been shown to improve expressiveness in GNN's by Papp et al \cite{NEURIPS2021_b8b2926b}.

\section{Training and fine-tuning}

\subsection{Data division}
For the purpose of model validation, we used 5-fold cross-validation, thus splitting the training set described in \myworries{ TODOOO Numer sekcji} into 5 parts and training on 4 of those parts. This number of folds corresponds to the number of 300-character blocks, that the users were asked to input. Such splits validate that the trained model is able to generalize to a part of the input that might have been written at a different time and style. Furthermore, it closely resembles the way the model was tested, that is on a completely different input sequence, collected in a separate part of the application.  

\subsection{Training parameters}

\subsubsection{Loss function}
Binary Cross Entropy was chosen as the loss function that would be used in training, which is a common loss function used with binary classifiers. 
The use of this loss function requires the model to output the probability of picking the positive class, therefore a sigmoid function must be applied to the output of the final layer.
These to operations are fused into one step in the implementation, by using  $BCEWithLogitsLoss$, which is more numerically stable.

\subsubsection{Optimizer}
Adam was selected as the optimizer for updating model weights, which adapts the learning rate for each parameter. A learning rate of 0.001 was chosen empirically.

\subsection{Tuning hyperparameters}
\myworries{TODO: Nie wiem czy tutaj czy moze w sekcji z wynikami tutaj to za bardzo nie ma sensu. Moge tu napisać ale chyba to nie ma sensu, lepiej jest chyba omówić wpływ parametru na wynik. Z sekcji Testing model on users bym zrobił chapter}
