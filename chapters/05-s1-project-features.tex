
\section{Choosing features for Neural Network model}
\myworries{TODO Some introdutions}


\subsection{Data exploration}
\myworries{All data analisys on the input goes here}

\subsection{Graph creation and feature encoding}
The input for graph creation consists of two main parts, the duration of time between individual key presses, and the character of the key that was pressed.
A natural way to represent such input was to map each unique character in the input sequence to a node in the graph. 
Directed edges were added between nodes that represent characters appearing after each other in the input sequence. 
\myworries{TODO: add example graph visualization here}
Each time the same pair of characters appears in the input text, it maps to the same edge. For each such pair, the duration is added to a list of attributes for that edge, which will be aggregated in later stages, to a form suitable for the GCN model. It would also be possible to model such pairs using a multiedge graph, as such models have been shown to perform well in other domains \myworries{TODO zacytuj "multi-edge graph for convolutional networks for power systems}. However, we did not consider this approach. 

\myworries{TODO maybe visualization}
The structure of the resulting graph depends highly on the length of the input sequence. A shorter sequence produces smaller and sparser graphs, while a longer input sequences result in graphs with more nodes and edges. 

\myworries{Citation needed - some RNN paper that sequences of keys are important}\\

\subsubsection{edge atributes encoding}
In order for the graph to be a valid input for a GCN network, edge attributes need to be converted into node features. We found two ways to encode aggregate this information into node features.\\  
For each node $i$:
\begin{enumerate}
	\item Two values representing the average duration before and after the key represented by $i$ was pressed.
	\item Add two-dimensional vector of values, of size [number of allowed characters, 2]. Each key that can be found in the input is assigned a number. The $n$'th row in the vector corresponds to a node, with a key assigned the number $n$, now called node $j$. The $n$'th row contains two values: the average duration on the edge from $i$ to $j$ and the average duration on the edge from $j$ to $i$. The values for which edges do not exist were assigned 0.
\end{enumerate}
The clear difference between these two approaches is the level of aggregation. Method $1$ aggregates all the edge information into 2 values, while method $2$ aggregates it into a vector of values, although it imposes some limitations, such as assigning each key a unique index into this input vector. Furthermore, method $2$ increases the overall size of the input data and complexity of the model.


\subsubsection{Character attributes encoding}
As this project focuses on recognizing users of the Polish language, the character encodings were designed to make use of this fact. For the purpose of encoding, characters were divided into several groups:\\
\begin{itemize}
	\item Letters -- characters \textit{a-z}, including diacritics.
	\item Numbers -- characters \textit{0-9}
	\item Special characters -- \textit{space}, \textit{tab}, \textit{newline}, \textit{backspace}, \textit{dot}, \textit{comma}, \textit{exclamation mark}, \textit{question mark}.
	\item Symbols -- \textit{*}, \textit{\#}, \textit{@}, \textit{:}, \textit{;},   \textit{'}, \textit{"}, \textit{(}, \textit{)}, \textit{[}, \textit{]}, \textit{\{}, \textit{\}}, \textless, \textgreater, \textit{/}, \textit{$\backslash$}, \textit{|}, \&, \%, \textit{\$}, \textit{\^}, \~, \textit{\_}, \textit{+}, \textit{-}, \textit{=}. 
	\item Others -- all other symbols.
\end{itemize}
Similarly, there is more than one way to encode key information into node features.
We considered three methods, all being variations on a one-hot encoding of keys:
\begin{enumerate}
	\item Classic one-hot representation. Each unique cased character is mapped to different column in the vector, except for characters in the \textit{Others} group, which map to one extra column.
	\item Small alphabet representation. One hot encoding for cased \textit{Letters} and \textit{Special characters}. \textit{Numbers} map to one column in the vector, \textit{Symbols} and \textit{Others} map to one column in the vector.
	\item Base letter representation. All \textit{Letters} are converted to lower case, diacritical marks are removed. These transformed letters are encoded in a one-hot vector. \textit{Numbers} map to one column in the vector, \textit{Symbols} and \textit{Others} map to one column in the vector. Two additional columns are added to the input, one indicates whether the original character was a capitalized letter, the second whether it had a diacritical mark.
\end{enumerate}

\myworries{Citation needed: That paper that said node ids are nice}.
Again as before, these methods differ by degree of aggregation. Methods $2$ and $3$ group certain letters together, mapping multiple characters to the same values, while method $1$ provides a unique, one hot encoded identifier for each node. \myworries{Cication} found that such node identifiers helped the model to learn certain structures in the data. However, \myworries{Slajdy z stanfordu ze tylko jak jest ograniczona liczba liter} notes, that providing node identifiers as input features work well only for a small and known set of possible input nodes. While this requirement appears to hold true for this specific task, we found this is not the case. Many letters, which appear to be common, in fact do not appear in our dataset. \myworries{TODO Which chars never appear}. This means that the behavior of the models would be unpredictable for nodes with such identifiers.
Some characters, for example \myworries{EXMAPLEEEE like '('}, appear only once, leading models to overfit and generalize poorly.
Method $3$ was specifically designed to deal with the low number of appearances of certain characters, especially uppercase letters. However, the extra column still allows for distinguishing between lower and uppercase variants of the same letter, which is important as the time to input these symbol differs greatly. \myworries{Avg time for uppercase and lowercase letters}
Moreover, method $3$ directly exposes the use of diacritical marks, which, similarly to uppercase letter, take longer to input \myworries{Gimme DATAAAAA}. 
\myworries{Ładne zdanie które mówi wooow, diacritical marks are nice in polish, maybe data}

\myworries{ADD histogram of how many times each character appears}\\
\myworries{ADD Which chars never appear}


\subsection{Feature selection -- accelerometer data}
To improve the possible performance of the model, and to make further use of the capabilities of the mobile platform, we considered using accelerometer data as an input feature for the model.
This portion of the input data comprised of three values, a measurement of the acceleration in the x, y and z plane at the moment a keystroke was registered. 
These values were aggregated as an average for each node. Although these models performed well during training, quickly reaching low loss values, they failed to generalize, performing worse on 
validation and test datasets. 

\myworries{TODO, some accelerometer data here}
For this reason, accelerometer data was not used for training and evaluating models discussed later.



